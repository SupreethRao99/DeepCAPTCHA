{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "DeepCAPTCHA-GPU.ipynb",
      "provenance": [],
      "mount_file_id": "1oahmAcbdXZS0wB0bV6MGXCzGLY97YY68",
      "authorship_tag": "ABX9TyM3EWq0/UfOXbdg+9xUZUiU",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SupreethRao99/DeepCAPTCHA/blob/main/DeepCAPTCHA_GPU.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ndHc1SnuprQ3"
      },
      "source": [
        "# DeepCAPTCHA\n",
        "DeepCAPTHA is a ResNet architecture based convolutional neural network (CNN) trained on the [Chars74K-Fonts](http://www.ee.surrey.ac.uk/CVSSP/demos/chars74k/#download) Dataset. It has been built as part of a larger project which attempts to defeat simple CAPTCHAs. The Network trained in this notebook achieves a training accuracy of 89.41% and a validation accuracy of 88.00%."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uw1eAPbTU02E"
      },
      "source": [
        "The [dataset](https://www.kaggle.com/supreethrao/chars74kdigitalenglishfont) used in this notebook can also be found on kaggle"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pdqEFYv93qM9"
      },
      "source": [
        "# importing the required libraries\n",
        "from google.colab import drive\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "import numpy as np\n",
        "import zipfile\n",
        "import os\n",
        "import random\n",
        "from shutil import copyfile\n",
        "import datetime"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B9c8oiJKSqVm",
        "outputId": "dbd978ac-62b7-4768-dfb1-552df04408bf"
      },
      "source": [
        "!nvidia-smi #displays the GPU allocated by google colab"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sun Apr  4 16:39:26 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.67       Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   34C    P0    25W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vL1T5sO2rAiF"
      },
      "source": [
        "The dataset is stored on google drive. The dataset is then loaded onto colab and unzipped. training and testing directories are created for each class present in the dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XvTsFgYPCQBg",
        "outputId": "da3e2683-389c-412c-d5fe-72fba8c181ea"
      },
      "source": [
        "drive.mount('/content/drive') "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VZxhm3R3BuPZ"
      },
      "source": [
        "zip_ref = zipfile.ZipFile(\"/content/drive/MyDrive/English.zip\", 'r')\n",
        "zip_ref.extractall(\"/tmp\")\n",
        "zip_ref.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d5Sies0tLdjh"
      },
      "source": [
        "# Training and testing directories are created with directory name being the\n",
        "# label of the class. eg:images of the letter 'A' will be in directory 'A'\n",
        "# similarly images of letter 'a' will be in directory 'a'.\n",
        "\n",
        "# Note that directory names are case-sensitive.\n",
        "os.mkdir('/tmp/CAPTCHA')\n",
        "os.mkdir('/tmp/CAPTCHA/testing')\n",
        "os.mkdir('/tmp/CAPTCHA/training')\n",
        "\n",
        "for i in range(0,62):\n",
        "  try:\n",
        "    if i>= 0 and i<10: # for numbers 0-9\n",
        "      os.mkdir('/tmp/CAPTCHA/training/'+chr(i+48))\n",
        "      os.mkdir('/tmp/CAPTCHA/testing/'+chr(i+48))\n",
        "\n",
        "    if i>= 10 and i<36: # for alphabets A-Z\n",
        "      os.mkdir('/tmp/CAPTCHA/training/'+chr(i-10+65))\n",
        "      os.mkdir('/tmp/CAPTCHA/testing/'+chr(i-10+65))\n",
        "      \n",
        "    if i>=36 and i<62: # for alphabets a-z\n",
        "      os.mkdir('/tmp/CAPTCHA/training/'+chr(i-36+97))\n",
        "      os.mkdir('/tmp/CAPTCHA/testing/'+chr(i-36+97))\n",
        "      \n",
        "  except OSError:\n",
        "    print('directory creation failed')\n",
        "    pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LTBt5cd2raSd"
      },
      "source": [
        "the `split_data` function splits the dataset into training and testing sets randomly. the size of the training and testing set is determined by the `SPLIT_SIZE` parameter"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gWjftMSCKJtj"
      },
      "source": [
        "from random import shuffle\n",
        "import shutil\n",
        "\n",
        "def split_data(SOURCE, TRAINING, TESTING, SPLIT_SIZE):\n",
        "    all_images = os.listdir(SOURCE)\n",
        "    shuffle(all_images)\n",
        "    splitting_index = round(SPLIT_SIZE*len(all_images))\n",
        "    train_images = all_images[:splitting_index]\n",
        "    test_images = all_images[splitting_index:]\n",
        "\n",
        "    for img in train_images:\n",
        "        src = os.path.join(SOURCE, img)\n",
        "        dst = os.path.join(TRAINING, img)\n",
        "        if os.path.getsize(src) <= 0:\n",
        "            print(img+\" is zero length, so ignoring!!\")\n",
        "        else:\n",
        "            shutil.copyfile(src, dst)\n",
        "\n",
        "    for img in test_images:\n",
        "        src = os.path.join(SOURCE, img)\n",
        "        dst = os.path.join(TESTING, img)\n",
        "        if os.path.getsize(src) <= 0:\n",
        "            print(img+\" is zero length, so ignoring!!\")\n",
        "        else:\n",
        "            shutil.copyfile(src, dst)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U9H_hfeTKsI8"
      },
      "source": [
        "split_size = 0.90\n",
        "for i in range(0,62):\n",
        "  if i>=0 and i<10:\n",
        "    split_data('/tmp/English/Fnt/'+chr(i+48),\n",
        "               '/tmp/CAPTCHA/training/'+chr(i+48),\n",
        "               '/tmp/CAPTCHA/testing/'+chr(i+48),\n",
        "               split_size)\n",
        "  if i>=10 and i<36:\n",
        "    split_data('/tmp/English/Fnt/'+chr(i-10+65)+\"-1\",\n",
        "               '/tmp/CAPTCHA/training/'+chr(i-10+65),\n",
        "               '/tmp/CAPTCHA/testing/'+chr(i-10+65),\n",
        "               split_size)\n",
        "  if i>=36 and i<62:\n",
        "    split_data('/tmp/English/Fnt/'+chr(i-36+97)+\"-2\",\n",
        "               '/tmp/CAPTCHA/training/'+chr(i-36+97),\n",
        "               '/tmp/CAPTCHA/testing/'+chr(i-36+97),\n",
        "               split_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cSvUg3QzrClp"
      },
      "source": [
        "# Creation of Model\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XwqnXOF_bWfe"
      },
      "source": [
        "%load_ext tensorboard\n",
        "%tensorboard --logdir logs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rA1eAnP3sVrM"
      },
      "source": [
        "Images are augmented by rescaling, rotating , shearing, zooming and flipping. This provides a cheap and very effective way to provide more data for the model to learn from."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JQMC1ai1u9sm",
        "outputId": "3239df09-779f-4d36-a018-e66acd6368b4"
      },
      "source": [
        "TRAINING_DIR = '/tmp/CAPTCHA/training'\n",
        "train_datagen = ImageDataGenerator(rescale=1. / 255,\n",
        "                                   rotation_range=30,\n",
        "                                   shear_range=0.2,\n",
        "                                   zoom_range=0.2,\n",
        "                                   horizontal_flip=True,\n",
        "                                   fill_mode='nearest',\n",
        "            preprocessing_function = tf.image.rgb_to_grayscale)\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    TRAINING_DIR,\n",
        "    target_size = (32,32),\n",
        "    batch_size = 16,\n",
        "    class_mode = 'categorical'\n",
        ")\n",
        "\n",
        "VALIDATION_DIR = '/tmp/CAPTCHA/testing'\n",
        "validation_datagen = ImageDataGenerator(rescale=1./255,\n",
        "            preprocessing_function = tf.image.rgb_to_grayscale)\n",
        "validation_generator = validation_datagen.flow_from_directory(\n",
        "    VALIDATION_DIR,\n",
        "    target_size = (32,32),\n",
        "    batch_size = 16,\n",
        "    class_mode = 'categorical'\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 56668 images belonging to 62 classes.\n",
            "Found 6324 images belonging to 62 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3n_Ac88ls0bX"
      },
      "source": [
        "## The ResNet Model\n",
        "the model described below uses a custom model based on the [ResNet architecture](https://arxiv.org/pdf/1512.03385.pdf) "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NEHqNmrg588-"
      },
      "source": [
        "import keras\n",
        "from functools import partial\n",
        "DefaultConv2D = partial(keras.layers.Conv2D, kernel_size=3, strides=1,\n",
        "                        padding=\"SAME\", use_bias=False)\n",
        "\n",
        "class ResidualUnit(keras.layers.Layer):\n",
        "    def __init__(self, filters, strides=1, activation=\"relu\", **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.activation = keras.activations.get(activation)\n",
        "        self.main_layers = [\n",
        "            DefaultConv2D(filters, strides=strides),\n",
        "            keras.layers.BatchNormalization(),\n",
        "            self.activation,\n",
        "            DefaultConv2D(filters),\n",
        "            keras.layers.BatchNormalization()]\n",
        "        self.skip_layers = []\n",
        "        if strides > 1:\n",
        "            self.skip_layers = [\n",
        "                DefaultConv2D(filters, kernel_size=1, strides=strides),\n",
        "                keras.layers.BatchNormalization()]\n",
        "\n",
        "    def get_config(self):\n",
        "      cfg = super().get_config()\n",
        "      return cfg  \n",
        "\n",
        "    def call(self, inputs):\n",
        "        Z = inputs\n",
        "        for layer in self.main_layers:\n",
        "            Z = layer(Z)\n",
        "        skip_Z = inputs\n",
        "        for layer in self.skip_layers:\n",
        "            skip_Z = layer(skip_Z)\n",
        "        return self.activation(Z + skip_Z)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aX2FYSbN8Uf7",
        "outputId": "818829f2-8c29-4c68-a756-a47868bcb057"
      },
      "source": [
        "keras.backend.clear_session()\n",
        "tf.random.set_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "model = keras.models.Sequential()\n",
        "model.add(DefaultConv2D(64, kernel_size=4, strides=2,\n",
        "                        input_shape=[32, 32, 3]))\n",
        "model.add(keras.layers.BatchNormalization())\n",
        "model.add(keras.layers.Activation(\"relu\"))\n",
        "model.add(keras.layers.MaxPool2D(pool_size=3, strides=2, padding=\"SAME\"))\n",
        "prev_filters = 64\n",
        "for filters in [64] * 2 + [128] * 2 + [256] * 2 :\n",
        "    strides = 1 if filters == prev_filters else 2\n",
        "    model.add(ResidualUnit(filters, strides=strides))\n",
        "    prev_filters = filters\n",
        "model.add(keras.layers.GlobalAvgPool2D())\n",
        "model.add(keras.layers.Flatten())\n",
        "model.add(keras.layers.Dropout(0.5))\n",
        "model.add(keras.layers.Dense(62, activation=\"softmax\"))\n",
        "\n",
        "model.compile(loss=\"categorical_crossentropy\", optimizer=\"nadam\",\n",
        "              metrics=[\"accuracy\"])\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d (Conv2D)              (None, 16, 16, 64)        3072      \n",
            "_________________________________________________________________\n",
            "batch_normalization (BatchNo (None, 16, 16, 64)        256       \n",
            "_________________________________________________________________\n",
            "activation (Activation)      (None, 16, 16, 64)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d (MaxPooling2D) (None, 8, 8, 64)          0         \n",
            "_________________________________________________________________\n",
            "residual_unit (ResidualUnit) (None, 8, 8, 64)          74240     \n",
            "_________________________________________________________________\n",
            "residual_unit_1 (ResidualUni (None, 8, 8, 64)          74240     \n",
            "_________________________________________________________________\n",
            "residual_unit_2 (ResidualUni (None, 4, 4, 128)         230912    \n",
            "_________________________________________________________________\n",
            "residual_unit_3 (ResidualUni (None, 4, 4, 128)         295936    \n",
            "_________________________________________________________________\n",
            "residual_unit_4 (ResidualUni (None, 2, 2, 256)         920576    \n",
            "_________________________________________________________________\n",
            "residual_unit_5 (ResidualUni (None, 2, 2, 256)         1181696   \n",
            "_________________________________________________________________\n",
            "global_average_pooling2d (Gl (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 62)                15934     \n",
            "=================================================================\n",
            "Total params: 2,796,862\n",
            "Trainable params: 2,792,382\n",
            "Non-trainable params: 4,480\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RIG7HCQ6AQ38"
      },
      "source": [
        "logdir = os.path.join(\"logs\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
        "tensorboard_callback = tf.keras.callbacks.TensorBoard(logdir, histogram_freq=1)\n",
        "\n",
        "# performance scheduling is used to reduce learning rate\n",
        "# to improve model training accuracy\n",
        "\n",
        "learning_rate_reduction = tf.keras.callbacks.ReduceLROnPlateau(\n",
        "    monitor='val_accuracy', factor=0.5, patience=2,min_lr=0.00001,mode='auto')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ngXcpo7m_iMg",
        "outputId": "a88fab7f-865b-46c9-fe54-5ef50727a6e2"
      },
      "source": [
        "history = model.fit(train_generator, epochs = 50,\n",
        "                    validation_data=validation_generator,\n",
        "          callbacks=[tensorboard_callback, learning_rate_reduction])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "3542/3542 [==============================] - 198s 47ms/step - loss: 2.1433 - accuracy: 0.4257 - val_loss: 1.0550 - val_accuracy: 0.6482\n",
            "Epoch 2/50\n",
            "3542/3542 [==============================] - 165s 46ms/step - loss: 0.8481 - accuracy: 0.7205 - val_loss: 0.6754 - val_accuracy: 0.7679\n",
            "Epoch 3/50\n",
            "3542/3542 [==============================] - 166s 47ms/step - loss: 0.7120 - accuracy: 0.7584 - val_loss: 0.5683 - val_accuracy: 0.7887\n",
            "Epoch 4/50\n",
            "3542/3542 [==============================] - 164s 46ms/step - loss: 0.6451 - accuracy: 0.7755 - val_loss: 0.5259 - val_accuracy: 0.8033\n",
            "Epoch 5/50\n",
            "3542/3542 [==============================] - 165s 47ms/step - loss: 0.5815 - accuracy: 0.7926 - val_loss: 0.5272 - val_accuracy: 0.8066\n",
            "Epoch 6/50\n",
            "3542/3542 [==============================] - 164s 46ms/step - loss: 0.5452 - accuracy: 0.8020 - val_loss: 0.5391 - val_accuracy: 0.8025\n",
            "Epoch 7/50\n",
            "3542/3542 [==============================] - 165s 46ms/step - loss: 0.5218 - accuracy: 0.8086 - val_loss: 0.4610 - val_accuracy: 0.8207\n",
            "Epoch 8/50\n",
            "3542/3542 [==============================] - 164s 46ms/step - loss: 0.5014 - accuracy: 0.8152 - val_loss: 0.4272 - val_accuracy: 0.8318\n",
            "Epoch 9/50\n",
            "3542/3542 [==============================] - 164s 46ms/step - loss: 0.4799 - accuracy: 0.8206 - val_loss: 0.4209 - val_accuracy: 0.8336\n",
            "Epoch 10/50\n",
            "3542/3542 [==============================] - 163s 46ms/step - loss: 0.4596 - accuracy: 0.8262 - val_loss: 0.4095 - val_accuracy: 0.8409\n",
            "Epoch 11/50\n",
            "3542/3542 [==============================] - 164s 46ms/step - loss: 0.4489 - accuracy: 0.8309 - val_loss: 0.4079 - val_accuracy: 0.8344\n",
            "Epoch 12/50\n",
            "3542/3542 [==============================] - 164s 46ms/step - loss: 0.4302 - accuracy: 0.8315 - val_loss: 0.3721 - val_accuracy: 0.8468\n",
            "Epoch 13/50\n",
            "3542/3542 [==============================] - 164s 46ms/step - loss: 0.4236 - accuracy: 0.8342 - val_loss: 0.3864 - val_accuracy: 0.8403\n",
            "Epoch 14/50\n",
            "3542/3542 [==============================] - 163s 46ms/step - loss: 0.4160 - accuracy: 0.8392 - val_loss: 0.3735 - val_accuracy: 0.8472\n",
            "Epoch 15/50\n",
            "3542/3542 [==============================] - 164s 46ms/step - loss: 0.4010 - accuracy: 0.8442 - val_loss: 0.3982 - val_accuracy: 0.8362\n",
            "Epoch 16/50\n",
            "3542/3542 [==============================] - 164s 46ms/step - loss: 0.3915 - accuracy: 0.8458 - val_loss: 0.3688 - val_accuracy: 0.8534\n",
            "Epoch 17/50\n",
            "3542/3542 [==============================] - 164s 46ms/step - loss: 0.3968 - accuracy: 0.8450 - val_loss: 0.3652 - val_accuracy: 0.8460\n",
            "Epoch 18/50\n",
            "3542/3542 [==============================] - 164s 46ms/step - loss: 0.3829 - accuracy: 0.8508 - val_loss: 0.3529 - val_accuracy: 0.8577\n",
            "Epoch 19/50\n",
            "3542/3542 [==============================] - 166s 47ms/step - loss: 0.3774 - accuracy: 0.8471 - val_loss: 0.3523 - val_accuracy: 0.8563\n",
            "Epoch 20/50\n",
            "3542/3542 [==============================] - 163s 46ms/step - loss: 0.3701 - accuracy: 0.8517 - val_loss: 0.3458 - val_accuracy: 0.8520\n",
            "Epoch 21/50\n",
            "3542/3542 [==============================] - 165s 46ms/step - loss: 0.3372 - accuracy: 0.8630 - val_loss: 0.3209 - val_accuracy: 0.8703\n",
            "Epoch 22/50\n",
            "3542/3542 [==============================] - 164s 46ms/step - loss: 0.3237 - accuracy: 0.8672 - val_loss: 0.3208 - val_accuracy: 0.8629\n",
            "Epoch 23/50\n",
            "3542/3542 [==============================] - 166s 47ms/step - loss: 0.3132 - accuracy: 0.8692 - val_loss: 0.3240 - val_accuracy: 0.8646\n",
            "Epoch 24/50\n",
            "3542/3542 [==============================] - 165s 47ms/step - loss: 0.2958 - accuracy: 0.8769 - val_loss: 0.3002 - val_accuracy: 0.8765\n",
            "Epoch 25/50\n",
            "3542/3542 [==============================] - 165s 47ms/step - loss: 0.2868 - accuracy: 0.8799 - val_loss: 0.2982 - val_accuracy: 0.8787\n",
            "Epoch 26/50\n",
            "3542/3542 [==============================] - 164s 46ms/step - loss: 0.2809 - accuracy: 0.8812 - val_loss: 0.3018 - val_accuracy: 0.8743\n",
            "Epoch 27/50\n",
            "3542/3542 [==============================] - 164s 46ms/step - loss: 0.2829 - accuracy: 0.8790 - val_loss: 0.2998 - val_accuracy: 0.8718\n",
            "Epoch 28/50\n",
            "3542/3542 [==============================] - 165s 47ms/step - loss: 0.2737 - accuracy: 0.8849 - val_loss: 0.2876 - val_accuracy: 0.8760\n",
            "Epoch 29/50\n",
            "3542/3542 [==============================] - 164s 46ms/step - loss: 0.2637 - accuracy: 0.8881 - val_loss: 0.2845 - val_accuracy: 0.8740\n",
            "Epoch 30/50\n",
            "3542/3542 [==============================] - 165s 47ms/step - loss: 0.2589 - accuracy: 0.8890 - val_loss: 0.2828 - val_accuracy: 0.8800\n",
            "Epoch 31/50\n",
            "3542/3542 [==============================] - 164s 46ms/step - loss: 0.2572 - accuracy: 0.8892 - val_loss: 0.2837 - val_accuracy: 0.8793\n",
            "Epoch 32/50\n",
            "3542/3542 [==============================] - 164s 46ms/step - loss: 0.2488 - accuracy: 0.8916 - val_loss: 0.2838 - val_accuracy: 0.8782\n",
            "Epoch 33/50\n",
            "3542/3542 [==============================] - 164s 46ms/step - loss: 0.2545 - accuracy: 0.8880 - val_loss: 0.2827 - val_accuracy: 0.8782\n",
            "Epoch 34/50\n",
            "3542/3542 [==============================] - 167s 47ms/step - loss: 0.2532 - accuracy: 0.8898 - val_loss: 0.2847 - val_accuracy: 0.8765\n",
            "Epoch 35/50\n",
            "3542/3542 [==============================] - 168s 48ms/step - loss: 0.2554 - accuracy: 0.8902 - val_loss: 0.2817 - val_accuracy: 0.8797\n",
            "Epoch 36/50\n",
            "3542/3542 [==============================] - 166s 47ms/step - loss: 0.2495 - accuracy: 0.8902 - val_loss: 0.2810 - val_accuracy: 0.8800\n",
            "Epoch 37/50\n",
            "3542/3542 [==============================] - 166s 47ms/step - loss: 0.2488 - accuracy: 0.8919 - val_loss: 0.2800 - val_accuracy: 0.8795\n",
            "Epoch 38/50\n",
            "3542/3542 [==============================] - 167s 47ms/step - loss: 0.2487 - accuracy: 0.8923 - val_loss: 0.2823 - val_accuracy: 0.8792\n",
            "Epoch 39/50\n",
            "3542/3542 [==============================] - 166s 47ms/step - loss: 0.2475 - accuracy: 0.8937 - val_loss: 0.2819 - val_accuracy: 0.8797\n",
            "Epoch 40/50\n",
            "3542/3542 [==============================] - 170s 48ms/step - loss: 0.2493 - accuracy: 0.8925 - val_loss: 0.2809 - val_accuracy: 0.8790\n",
            "Epoch 41/50\n",
            "3542/3542 [==============================] - 170s 48ms/step - loss: 0.2461 - accuracy: 0.8938 - val_loss: 0.2827 - val_accuracy: 0.8778\n",
            "Epoch 42/50\n",
            "3542/3542 [==============================] - 167s 47ms/step - loss: 0.2470 - accuracy: 0.8920 - val_loss: 0.2812 - val_accuracy: 0.8782\n",
            "Epoch 43/50\n",
            "3542/3542 [==============================] - 167s 47ms/step - loss: 0.2497 - accuracy: 0.8908 - val_loss: 0.2808 - val_accuracy: 0.8793\n",
            "Epoch 44/50\n",
            "3542/3542 [==============================] - 166s 47ms/step - loss: 0.2451 - accuracy: 0.8935 - val_loss: 0.2816 - val_accuracy: 0.8781\n",
            "Epoch 45/50\n",
            "3542/3542 [==============================] - 167s 47ms/step - loss: 0.2507 - accuracy: 0.8914 - val_loss: 0.2835 - val_accuracy: 0.8778\n",
            "Epoch 46/50\n",
            "3542/3542 [==============================] - 167s 47ms/step - loss: 0.2496 - accuracy: 0.8916 - val_loss: 0.2817 - val_accuracy: 0.8798\n",
            "Epoch 47/50\n",
            "3542/3542 [==============================] - 166s 47ms/step - loss: 0.2497 - accuracy: 0.8922 - val_loss: 0.2805 - val_accuracy: 0.8800\n",
            "Epoch 48/50\n",
            "3542/3542 [==============================] - 166s 47ms/step - loss: 0.2483 - accuracy: 0.8926 - val_loss: 0.2807 - val_accuracy: 0.8793\n",
            "Epoch 49/50\n",
            "3542/3542 [==============================] - 168s 47ms/step - loss: 0.2505 - accuracy: 0.8911 - val_loss: 0.2801 - val_accuracy: 0.8792\n",
            "Epoch 50/50\n",
            "3542/3542 [==============================] - 167s 47ms/step - loss: 0.2441 - accuracy: 0.8941 - val_loss: 0.2809 - val_accuracy: 0.8800\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5PDSPJePuidl"
      },
      "source": [
        "The model is saved in the `saved_model` format and downloaded for use as part of the larger project."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ughhHjrLg26E",
        "outputId": "77801dc4-93cb-4b8a-e519-2bfdc5b923f2"
      },
      "source": [
        "model.save('CAPTCHA-Model')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_1_layer_call_fn, conv2d_1_layer_call_and_return_conditional_losses, conv2d_2_layer_call_fn, conv2d_2_layer_call_and_return_conditional_losses, conv2d_3_layer_call_fn while saving (showing 5 of 70). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_1_layer_call_fn, conv2d_1_layer_call_and_return_conditional_losses, conv2d_2_layer_call_fn, conv2d_2_layer_call_and_return_conditional_losses, conv2d_3_layer_call_fn while saving (showing 5 of 70). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: CAPTCHA-Model/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: CAPTCHA-Model/assets\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "byXVltIymdV8",
        "outputId": "dbc88828-d840-48e3-c36c-f1da58faa7ed"
      },
      "source": [
        "!zip -r /content/CAPTCHA.zip /content/CAPTCHA-Model"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  adding: content/CAPTCHA-Model/ (stored 0%)\n",
            "  adding: content/CAPTCHA-Model/variables/ (stored 0%)\n",
            "  adding: content/CAPTCHA-Model/variables/variables.index (deflated 76%)\n",
            "  adding: content/CAPTCHA-Model/variables/variables.data-00000-of-00001 (deflated 8%)\n",
            "  adding: content/CAPTCHA-Model/saved_model.pb (deflated 91%)\n",
            "  adding: content/CAPTCHA-Model/assets/ (stored 0%)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "ZFAedABbm8VD",
        "outputId": "166dd19a-4586-40b3-e003-edf9b39dca63"
      },
      "source": [
        "from google.colab import files\n",
        "files.download(\"/content/CAPTCHA.zip\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_971e3d87-fa0a-48af-b7fb-20b390cf17fa\", \"CAPTCHA.zip\", 30999517)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}